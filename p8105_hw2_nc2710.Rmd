---
title: "p8105_hw2_nc2710"
author: "Nicole Comfort"
date: "10/2/2018"
output: github_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(ggridges)
library(readxl)
library(dplyr)
library(janitor)

# set options for figures 
# knitr::opts_chunk$set(
#   fig.width = 6,
#   fig.asp = .6,
#   out.width = "90%"
# )

theme_set(theme_bw() + theme(legend.position = "bottom"))

```

## Problem 1

This problem focuses on NYC transit data; in particular, information related to each entrance and exit for each subway station in NYC.

*Read and clean the data; retain line, station name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable (the ifelse or recode function may be useful).*

*Write a short paragraph about this dataset – explain briefly what variables the dataset contains, describe your data cleaning steps so far, and give the dimension (rows x columns) of the resulting dataset. Are these data tidy?*

```{r import data}

NYC_transit_data = read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>% # clean column names
  select(line:entry, vending, ada) %>% # retain only specified information
  mutate(entry_logical = recode(entry,
                       "YES" = TRUE,
                       "NO" = FALSE)) %>% # convert entry from character to logical variable
  select(-entry) # remove old entry character variable

```

The code above imports data regarding NYC Transit information related to each entrance and exit for each subway station in NYC. After importing the file using a relative path, I cleaned the names and selected the following variables to retain in the dataset: line, station name, station's latitude / longitude, routes served, entry (whether entry is allowed), vending (YES/NO for whether it is present), entrance type (Elevator, stair, easement, etc...), and ADA compliance (YES/NO). I then created a new variable using the recode funtion, named "entry_logical". This step converted the "entry" variable from a character (YES/NO) to a logical variable (TRUE/FALSE). I next removed the "entry" character variable from the dataset. This data cleaning process results in a dataframe composed of `r nrow(NYC_transit_data)` rows and `r ncol(NYC_transit_data)` columns. 

However, these data still are not "tidy" because they are in wide format where we have multiple variables for the routes from route1:route11 (and thus a lot of missing data); In reality, "route" itself should be a variable and the route number (1-11) assigned as a value. This would convert our data from wide to long format. 

```{r answer questions}

# how many distinct stations are there: 
nrow(distinct(NYC_transit_data, line, station_name))

# how many of those distinct stations are ADA compliant: 
ada_compl_stns = filter(NYC_transit_data, ada == "TRUE") %>% 
  distinct(line, station_name)
nrow(ada_compl_stns)

# proportion of station entrances / exits without vending that allow entrance:
no_vending = filter(NYC_transit_data, vending == "NO") # filter to examine only entrances/exits without vending
mean(no_vending$entry_logical) # take the proportion of those that do allow entrance (entry_logical = TRUE)

```

*Answer the following questions using these data*:

* How many distinct stations are there?

There are `r nrow(distinct(NYC_transit_data, line, station_name))` distinct stations. 

* How many stations are ADA compliant?

Of those distinct stations, `r nrow(ada_compl_stns)` are ADA compliant. 

* What proportion of station entrances / exits without vending allow entrance?

The proportion of station entrances / exits without vending that allow entrance is 0.3770492.

*Reformat data so that route number and route name are distinct variables.* 

```{r}

NYC_transit_data_long =
  gather(NYC_transit_data, key = route_number, value = route_name, route1:route11) # reformat using gather function 

```

*How many distinct stations serve the A train? Of the stations that serve the A train, how many are ADA compliant?*

60 distinct stations serve the A train. Of the stations that serve the A train, 17 are ADA compliant. 

```{r}

# how many distinct stations serve the A train: 
A_line =
  filter(NYC_transit_data_long, route_name == "A") %>% # filter to examine only stations serving A
  distinct(., line, station_name, .keep_all = TRUE) # how many distinct stations (line and station) serve A

nrow(A_line) # gives the number of distinct stations that serve the A line

# how many of these are ADA compliant:
filter(NYC_transit_data_long, route_name == "A") %>% # filter to examine only stations serving A
  distinct(., line, station_name, .keep_all = TRUE) %>% # how many distinct stations (line and station) serve A
  filter(., ada == TRUE) %>% # now see how many of these are ADA compliant 
  nrow() # number of rows gives the number ADA compliant

```

## Problem 2 

This problem uses the Mr. Trash Wheel dataset. 

*Read and clean the Mr. Trash Wheel sheet*:

* specify the sheet in the Excel file and to omit columns containing notes (using the range argument)

* use reasonable variable names

* omit rows that do not include dumpster-specific data

* rounds the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)

```{r import Mr. Trash Wheel data}

library(readxl)
library(janitor)

# read in Mr. Trash Wheel data using updated dataset 10/04/18
trash_data = read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx",
             range = "Mr. Trash Wheel!A2:N338", # specify sheet and omit column containing notes
             col_types = c("text", "guess", "text", "guess", "guess", "guess", "guess", "guess", "guess", "guess","guess", "guess","guess", "guess")) %>%  # specify column types
  clean_names() %>% # cleans names
  filter(!is.na(dumpster)) %>% # exclude rows that don't have dumpster-specific data
  mutate(sports_balls = round(sports_balls)) %>% # round no. sports balls to nearest integer
  mutate(sports_balls = as.integer(sports_balls)) # convert sports_balls variable to an integer

# median number of sports balls in 2016
sports_balls_2016 = filter(trash_data, year == "2016") # filter to examine only year 2016
median(sports_balls_2016$sports_balls, na.rm = TRUE) # take the median number of sports balls collected 

# weight of trash collected 2016 
trash_data_2016 = filter(trash_data, year == "2016")
sum(trash_data_2016$weight_tons)

# weight of trash collected 2017 
trash_data_2017 = filter(trash_data, year == "2017")
sum(trash_data_2017$weight_tons)
```

*Read and clean precipitation data for 2016 and 2017. For each, omit rows without precipitation data and add a variable year. Next, combine datasets and convert month to a character variable (the variable month.name is built into R and should be useful).*

```{r import precipitation data}

precip_data_16 =
  read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", # read in data
             range = "2016 Precipitation!A2:B14") %>% # specify sheet and range
  janitor::clean_names() %>% # clean names
  filter(!is.na(total)) %>% # omit rows without precipitation data 
  mutate(year = "2016") # add a variable for year 

precip_data_17 =
  read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx",
             range = "2017 Precipitation!A2:B14") %>% # specify sheet and range 
  janitor::clean_names() %>% # clean names
  filter(!is.na(total)) %>% # omit rows without precipitation data 
  mutate(year = "2017") # add a variable for year

# combine data sets
precip_data_tidy = 
  bind_rows(precip_data_16, precip_data_17) %>% # bind rows
  mutate(month = month.name[month]) # convert month to character variable 

# 2016 precipitation stats
mean(precip_data_16$total, na.rm = TRUE) # mean precipitation
sum(precip_data_16$total) # total precipitation

# 2017 precipitation stats
mean(precip_data_17$total, na.rm = TRUE) # mean precipitation
sum(precip_data_17$total) # total precipitation

```

*Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in both resulting datasets, and give examples of key variables. For available data, what was the total precipitation in 2017? What was the median number of sports balls in a dumpster in 2016?*

The Mr. Trash Weel dataset contains data collected about a water wheel (named "Mr. Trash Wheel") that removes trash from the Inner Harbor in Baltimore, Maryland. The data contains information about 285 specific dumpsters, the date of collection, the amount of total trash (including the weight in tons and volume in cubic yards) collected, and the type of trash collected (e.g. cigarette butts, plastic bottles, chip bags, and grocery bags). Thus, we can gather a lot of information about the types of trash moving into the harbor at specific years and times of year. For example, the median number of sports balls collected in a dumpster in 2016 was `r median(sports_balls_2016$sports_balls, na.rm = TRUE)`. Furthermore, this trash is then incinerated to provide energy for homes and the data contains information on the number of homes powered by the collected trash. The dataset is `r nrow(trash_data)` rows by `r ncol(trash_data)` columns. 

Because the amount of trash Mr. Trash Wheel receives is highly dependent on rainfall (i.e. large storms move a lot of trash from the street into the harbor), we also examined precipitation data for 2016 and 2017. These two data sets for 2016 and 2017 contain the total rainfall for a given month of that year as well as the total rainfall for the year. The 2016 precipitation dataset is `r nrow(precip_data_16)` rows by `r ncol(precip_data_16)` columns (key variables: month, total precipitation, year) and the 2017 precipitation dataset is also `r nrow(precip_data_17)` rows by `r ncol(precip_data_17)` columns. Combining these datasets thus yields a dataset `r nrow(precip_data_tidy)` rows by `r ncol(precip_data_tidy)` columns. In 2016, the total precipitation was `r sum(precip_data_16$total)` inches and the mean precipitation was `r mean(precip_data_16$total, na.rm = TRUE)` inches. In 2017, the total precipitation was `r sum(precip_data_17$total)` inches and the mean precipitation was `r mean(precip_data_17$total, na.rm = TRUE)` inches.

Yet interestingly, more trash was collected in 2017 than 2016. In 2016, there were `r sum(trash_data_2016$weight_tons)` tons of trash collected whereas in 2017 there were `r sum(trash_data_2017$weight_tons)` tons of trash collected. 

## Problem 3 

This problem uses data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010, accessed from data.gov in Sept 2018 and downloaded from the p8105 github.

The dataset initially contains 134,203 rows and 23 columns. There is information on location, topic, question, response, and response number. The data is structured so that each (multiple-choice) response to each question is a separate row. A complete data dictionary is available online.

```{r download dataset}

# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets", force = TRUE)

library("p8105.datasets", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
data("brfss_smart2010")

```

The below code chunk does the following: 

* formats the data to use appropriate variable names

* focuses on the “Overall Health” topic

* excludes variables for class, topic, question, sample size, and everything from lower confidence limit to GeoLocation

* structures data so that values for 'Response' (“Excellent” to “Poor”) are column names / variables which indicate the proportion of subjects with each response (which are values of 'Data_value' in the original dataset)

* creates a new variable showing the proportion of responses that were “Excellent” or “Very Good”

```{r brfss data cleaning}

brfss_smart2010 =
  janitor::clean_names(brfss_smart2010) %>% # format the data to use appropriate variable names
  filter(topic == "Overall Health") %>% # focus on the “Overall Health” topic
  select(year, locationabbr, locationdesc, response, data_value) %>% # exclude variables 
  spread(key = response, value = data_value) %>% # structure data so for responses to wide format
  janitor::clean_names() %>% 
  mutate(top_responses = (excellent + very_good)/(excellent + very_good + good + fair + poor)) # create new variable showing proportion of responses that are "excellent" or "very good" 

# which state is observed the most? 
getmode <- function(v) {   # create a function to get the mode (found online)
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

most_observed_state = getmode(brfss_smart2010$locationabbr) # which state is observed the most 
  
```

*Using this dataset, do or answer the following*:

* How many unique locations are included in the dataset? Is every state represented? What state is observed the most?

There are `r nrow(distinct(brfss_smart2010, locationdesc))` unique locations included in the dataset.

There are `r nrow(distinct(brfss_smart2010, locationabbr))` states represented, so yes, all states are represented as well as Washington, D.C. 

The state with the most observations in the dataset is New Jersey (NJ).  

```{r median}

brfss_2002 = filter(brfss_smart2010, year == "2002") # focus on year 2002 
median(brfss_2002$excellent, na.rm = TRUE) # take the median and save as a variable for later use

```

* In 2002, what is the median of the “Excellent” response value?

In 2002, the median of the "Excellent" response value is `r median(brfss_2002$excellent, na.rm = TRUE)`. 

* Make a histogram of “Excellent” response values in the year 2002.

```{r histogram}

ggplot(brfss_2002, aes(x = excellent)) + 
  geom_histogram() + # make a histogram
  labs(
    title = "Histogram of 'Excellent' Responses to Overall Health topic in 2002",
    x = "Proportion of Respondents to Respond 'Excellent'",
    y = "Count",
    caption = "Data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART), 2002"
  )

```

* Make a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County (both in NY State) in each year from 2002 to 2010.

```{r scatterplot}

# create new dataset which focuses on the NY counties of interest and creates a new variable for the proportion of "excellent" response values 
brfss_NY = filter(brfss_smart2010,
                  locationdesc == "NY - New York County" | locationdesc == "NY - Queens County") %>% 
           mutate(prop_excellent = (excellent)/(excellent + very_good + good + fair + poor)) %>% 
           dplyr::rename(County = locationdesc)

# make scatterplot
ggplot(brfss_NY, aes(x = year, y = prop_excellent)) + 
  geom_point(aes(color = County)) + 
  labs(
    title = "'Excellent' Overall Health Respondents in NY Counties",
    x = "Year",
    y = "Proportion of Respondents to Respond 'Excellent'",
    caption = "Data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART), 2002-2010, accessed from data.gov in Sept 2018"
  )

```



